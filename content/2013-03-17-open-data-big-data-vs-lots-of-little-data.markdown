---
author: Craig
comments: true
date: 2013-03-17 09:18:10+00:00
layout: post
category: essays
slug: open-data-big-data-vs-lots-of-little-data
title: Open Data - Big Data vs. Lots of Little Data
---


Most people who deal with scientific data on a regular basis have likely run into a situation where they didn't have the "right" program installed on their computer to read a particular data file format which they needed to analyze right away.  Granted, most people who have computers - regardless of profession - have had this problem, but I'm recalling particularly miserable experiences as a graduate student researcher in experimental condensed matter physics -  feelings that, I hope, relate to those of other experimental scientists and some theoreticians as well.  Continuing with the painful memories:  perhaps, you had to leave the relative comfort of your office and _your_ computer, go seek out an ageing workstation which has the only licensed copy of the "right" proprietary software, located in some dank lab with no windows (well, maybe it had Windows... 2000...if you were lucky!).  You spend a few hours wrestling with the vendor's crappy analysis suite and finally export the boiled-down data as a spreadsheet format.... OK, almost home free.  Now, this computer most certainly had onerous restrictions to prevent all those nasty viruses from taking over and lowering your scientific productivity:  "no outside Internet connection, no USB ports, a floppy drive... forget it...  uh, what's the name of that FTP server again?"  You get back to your office, pull down that text file and finish the job with your favorite general purpose scientific analysis suite.  "Oh, wait... I need to know what that parameter was and it's not in the bleepin file!"

_<!-- more -->_

In certain technical fields like Condensed Matter Physics, Materials Science, Chemistry, Engineering, etc., practitioners have to deal with as many data file formats as there are different scientific instruments which they use - a number which is steadily increasing.  This proliferation of heterogeneous proprietary data formats is at least one major reason why the "informatics revolution" hasn't yet rocked these fields like it has in the _Big Data_ domains of Particle Physics, Genetics, and soon Medicine.  Well, clearly, _Big Science_ has big organizations behind it, directing the major mission(s) of the participants, driving collaborations, writing standards, making data open and available not by whim, or ideology, but out of necessity.  It is not by coincidence that data-sharing technologies like HTTP/HTML (the first website was at CERN, _the_ big European particle physics lab) and the makings of an early Internet (ARPANET set up by US Defense Department funded researchers) were forged in the environs of large-scale collaborative science.  On the other hand, the competitive stridency of smaller academic and corporate laboratories, while indisputably fostering innovation, may also be holding progress back to some extent.  It's in these areas where the data tends to remain little and hidden from view of the public, only to be boiled-down for and glossed over in flashy research reports.  The increasingly frequent revelations of rampant scientific fraud are a dour reminder that this system might be breaking down/already broken.

No one can doubt that the Internet has changed the way pretty much everyone in the 'First World', 'developed world', or 'global north' - certainly anyone reading this - lives, works, and plays.  The open standardization of Web technologies has been a catalyst for innovation and commerce; similarly, the collaborative, and sometimes competitive, but mostly transparent development methodologies of the Open Source Software movement, arguably, have led to technologies in use in virtually every new gadget on the market.  If any lessons are to be learned from these transformative endeavors, it's that the more eyes looking at the code/designs/data the better are the outcomes of the project and the faster is the pace of innovation.

The _Little Data_ fields are slowly beginning to see the value of openness.  It's become relatively inexpensive to post raw data files and to aggregate repositories of data from many groups.  But what incentives do these groups have to give away their secrets, especially if they are not confident that their results are repeatable/mistake free?  Entrenched norms of academic competition and reputation present barriers to genuine openness and transparency that are hard to overcome in these institutionally conservative climates.   It is a sort of inverse (perverse?) Prisoners' Dilemma where selfish agents could potentially attain more benefit by holding out while their competitors fess up; but, that analysis is a little too simple, because openness might lead to mutually beneficial collaborations between parties who are not at odds.  Even if every group reveals all the relevant data for published and even unpublished studies, how will the whole of this resource be greater than it parts - especially, if it is just a pile of hopelessly heterogeneous proprietary bit buckets with restricted access?

Search has proven to be a powerful tool for extracting useful information from huge collections of loosely structured data.  This works so well on the Web because all resources that are intended to be found (and some that are not) are discoverable through open-access linkages and expressed in a relatively small finite set of standard formats.  Individual scientific data sets for related experimental techniques often have comparable structures which are expressed within existing conventions of the literature.  However, a scientific study as a research article or an on-line journal is more loosely structured like web pages - and they often _are_ web pages these days, though not always discoverable by anyone (i.e., access could be restricted).  Machine readable annotations, either deliberately submitted by the authors or generated by machine learning algorithms, can enable a more powerful _semantic search_ that factors in context, specialized meanings, and interrelations of terms.  XML or other markup based formats together with a standardized set of terms especially important to the field of research, a so-called _ontology_, are popular technologies for embedding semantic annotation.

Lots of _Little Data_ can quickly become _Big Data_, so it is important to use formats that are as efficient as possible in storage and access-time, maybe with some trade-off depending on the specifics of the anticipated use cases.  Tabular data formats are extremely prevalent and can be encoded efficiently in binary database formats and further optimized with compression algorithms.   But, often the natural structure for some experiments is not easily forced into a 2-dimensional grid of primary data types, but may be better described by more complex entities such as n-dimensional arrays, compound tables of tables, or tree-like hierarchical structures.  There has long been freely available open source relational database technologies, such as SQL servers, which can efficiently represent complex relationships between items in tables with different compound data types and can extract values matching sophisticated queries; however, while these systems excel as data storage components for certain tasks such as business logistics, market segmentation, dynamic website back-ends... they, typically, are not optimal for the curation of scientific data sets, which benefit more from sequentially accessible complex structures.

Best practices suggest that any official record of an experiment should contain as many details, such as instrument settings and sample properties, that are needed to replicate it.  Other important details should also be documented, such as the time, location, people involved, etc. that contextualize the history of the experimental data set - or to borrow a term from the curation of historical artifacts, the "provenance".  We'll refer to these extra bits of information as _metadata_.  Just as this metadata could prove useful to an expert reviewer (or, even better, an on-line community) in assessing the worth of a given piece of science, these terms, given an _ontology_, could form a basis for _semantic discovery_ of the data set and its relations to other data sets in a greater database or even a set of databases with overlapping ontologies.

So, with these aims in mind, we at PVOS, want an open, standardized database-like format that can handle complex data structures in an efficient way  - while also allowing for the embedding of semantic metadata - to be adopted universally in experimental sciences as _the_ _Open Data_ standard  (or at least given a fair trial).  Luckily, most of the hard work has been done already by smart people who know _Big Data_: one very promising format standard for _Open Data_ is [HDF5](http://www.hdfgroup.org/HDF5/).  Innovative participants in the _Open Data_ software free-market have even made free and open tools available on many platforms, which make using this complex, high performance format nearly as simple as writing and accessing a spreadsheet.  Of course, the real advantages don't usually come from people manually poking around in ever larger spreadsheets but from facilitating rapid complex queries, cutting-edge data visualizations, and even unsupervised machine learning algorithms.  The [PyTables](http://www.pytables.org/) project has developed a slick interface for programmatic access and creation of HDF5 format databases using the open source language Python.  There are no longer any good reasons why the _Little Data_ fields can't bootstrap their informatics aspirations from the hard-won tools of _Big Data_ fields and, one day, be woven together in a grand _semantic web_ of scientific knowledge.

---

Craig Versek
